{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "676114ae",
      "metadata": {
        "id": "676114ae"
      },
      "source": [
        "## Every Dream trainer\n",
        "\n",
        "You will need your data prepared first before starting!  Don't waste rental fees if you're not ready to upload your files.  Your files should be captioned before you start with either the caption as the filename or in text files for each image alongside the image files.  See main README.md for more details. Tools are available to automatically caption your files.\n",
        "\n",
        "[Instructions](https://github.com/victorchall/EveryDream-trainer/blob/main/README.md)\n",
        "\n",
        "If you can sign up for Runpod here (shameless referral link): [Runpod](https://runpod.io?ref=oko38cd0)\n",
        "\n",
        "If you are confused by the wall of text, join the discord here: [EveryDream Discord](https://discord.gg/uheqxU6sXN)\n",
        "\n",
        "Make sure you have at least 40GB of Runpod **Volume** storage at a minimum so you don't waste training just 1 ckpt that is overtrained and have to start over.  Penny pinching on storage is ultimately a waste of your time and money!  This is setup to give you more than one ckpt so you don't overtrain.\n",
        "\n",
        "### Starting model\n",
        "Make sure you have your hugging face token ready to download the 1.5 mode. You can get one here: https://huggingface.co/settings/tokens\n",
        "If you don't have a User Access Token, create one.  Or you can upload a starting checkpoint instead of using the HF download and skip that step, but you'll need to modify the starting model name when you start training (more info below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0902e735",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0902e735",
        "outputId": "830703a1-4aa5-496e-ad00-e7d92d5634d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SwapCached:            0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Sun Dec  4 01:11:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check system resources, make sure your GPU actually has 24GB\n",
        "# You should see \"0 MB / 24576 MB\" in the middle of the printout\n",
        "# if you see 0 MB / 22000 MB find a different instance or provider...\n",
        "!grep Swap /proc/meminfo\n",
        "!swapon -s\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bb6d14b7-3c37-4ec4-8559-16b4e9b8dd18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb6d14b7-3c37-4ec4-8559-16b4e9b8dd18",
        "outputId": "377a05ac-6387-42b8-9862-4cff50a8954a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory ‘/content/drive/MyDrive/EveryDreamCkpts’: File exists\n",
            "Cloning into 'everydream-trainer'...\n",
            "remote: Enumerating objects: 1246, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 1246 (delta 92), reused 150 (delta 87), pack-reused 1085\u001b[K\n",
            "Receiving objects: 100% (1246/1246), 50.31 MiB | 15.99 MiB/s, done.\n",
            "Resolving deltas: 100% (733/733), done.\n",
            "/content/everydream-trainer\n",
            "DONE\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir /content/drive/MyDrive/EveryDreamCkpts\n",
        "!git clone https://github.com/victorchall/everydream-trainer\n",
        "%cd everydream-trainer\n",
        "\n",
        "finish_msg = codecs.encode('QBAR', 'rot_13')\n",
        "\n",
        "print(finish_msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589bfca0",
      "metadata": {
        "tags": [],
        "id": "589bfca0"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "**This will take a couple minutes.  Wait until it says \"DONE\" to move on.** \n",
        "You can ignore \"warnings.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ab559338",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab559338",
        "outputId": "9e4ef539-3efe-4b87-8b6d-2703f3b07415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 79 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 117 kB 90.6 MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 41 kB 463 kB/s \n",
            "\u001b[K     |████████████████████████████████| 585 kB 25.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 512 kB 80.3 MB/s \n",
            "\u001b[?25h  Building wheel for test-tube (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 31.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 92.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 79.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 551 kB 35.0 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
            "  Cloning https://github.com/CompVis/taming-transformers.git (to revision master) to ./src/taming-transformers\n",
            "  Running command git clone -q https://github.com/CompVis/taming-transformers.git /content/everydream-trainer/src/taming-transformers\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from taming-transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->taming-transformers) (4.1.1)\n",
            "Installing collected packages: taming-transformers\n",
            "  Running setup.py develop for taming-transformers\n",
            "Successfully installed taming-transformers-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip\n",
            "  Cloning https://github.com/openai/CLIP.git (to revision main) to ./src/clip\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /content/everydream-trainer/src/clip\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip) (2022.9.24)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "\u001b[K     |████████████████████████████████| 952 kB 31.6 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 34.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 329 kB 27.0 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/everydream-trainer\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from latent-diffusion==0.0.1) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->latent-diffusion==0.0.1) (4.1.1)\n",
            "Installing collected packages: latent-diffusion\n",
            "  Running setup.py develop for latent-diffusion\n",
            "Successfully installed latent-diffusion-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.8/dist-packages (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.64.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipywidgets==7.7.1 in /usr/local/lib/python3.8/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.7.1) (5.3.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.7.1) (3.6.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.7.1) (5.1.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.7.1) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.7.1) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.7.1) (7.9.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.7.1) (6.0.4)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (59.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 25.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.7.1) (4.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets==7.7.1) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.7.1) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.7.1) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (5.7.16)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (23.2.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.15.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.13.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (4.11.2)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (5.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (5.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2<=3.0.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets==7.7.1) (2.8.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (5.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.8.4)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (4.3.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.19.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (3.10.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets==7.7.1) (0.5.1)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n",
            "DONE\n"
          ]
        }
      ],
      "source": [
        "!pip install -q omegaconf\n",
        "!pip install -q einops\n",
        "!pip install -q pytorch-lightning==1.6.5\n",
        "!pip install -q test-tube\n",
        "!pip install -q transformers==4.19.2\n",
        "!pip install -q kornia\n",
        "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!pip install -q setuptools==59.5.0\n",
        "!pip install -q pillow==9.0.1\n",
        "!pip install -q torchmetrics==0.6.0\n",
        "!pip install -e .\n",
        "!pip install huggingface_hub\n",
        "!pip install ipywidgets==7.7.1\n",
        "import ipywidgets as widgets\n",
        "print(finish_msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c230d91a",
      "metadata": {
        "id": "c230d91a"
      },
      "source": [
        "## Now that dependencies are installed, ready to move on!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17affc47",
      "metadata": {
        "id": "17affc47"
      },
      "source": [
        "## Log into huggingface\n",
        "Run the cell below and paste your token into the prompt.  You can get your token from your [huggingface account page](https://huggingface.co/settings/tokens).\n",
        "\n",
        "The token will not show on the screen, just press enter after you paste it.\n",
        "\n",
        "Then run the following cell to download the base checkpoint (may take a minute)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "02c8583e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "920086eff21248e48d39e1445d8f9856",
            "83faf10b9bce4a269f453a5d50706d51",
            "4b32a734fcfa4813a323e67043dda0a2",
            "c662c77edca44e2fb34f88bc8b1944e3",
            "513ae98436d144978242c38d18553058",
            "cc2e8824c1a14226925ba9b6529a2d54",
            "0c7e2e32f6bc49fa9d93b1cb75f6324f",
            "1026949927f648d18634a5f1f248e694",
            "b89c77e6c2e5444e8b1642187cf6d7c2",
            "9c0a87e8ed0b4f3e9c30f385228084da",
            "50e47d3d1b984424b06a9f38d83e7a69",
            "429e9e25849a48fd8b12469f14c434f6",
            "9ac4369ef42c4d329e4c3d674bf00f62",
            "af661d4d10ea4ebbbab9e9e9df9ccfe4",
            "d5b3d673fa92457bb818f04e98da5203",
            "dae2a73653c54c189a78cf0379f083a7",
            "197d679123f14f21836c6a7ac49f04ce"
          ]
        },
        "id": "02c8583e",
        "outputId": "f98bf60b-4e2f-4194-84c1-85c314ed31a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "503322f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "503322f5",
        "outputId": "fdc4198a-024e-4875-83b9-cfcb2692ed81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--panopstor--EveryDream/snapshots/c768f08eebba76b42fcebba817f2643885202c81/sd_v1-5_vae.ckpt\n",
            "DONE\n"
          ]
        }
      ],
      "source": [
        "# Default is 1.5 with the new VAE, you can change this to another file on huggingface if you want:\n",
        "from huggingface_hub import hf_hub_download\n",
        "downloaded_model_path = hf_hub_download(\n",
        " repo_id=\"panopstor/EveryDream\", #@param \n",
        " filename=\"sd_v1-5_vae.ckpt\", #@param \n",
        " use_auth_token=True\n",
        ")\n",
        "\n",
        "#@markdown Copy Model To Gdrive to save time in Following runs and space on your instance\n",
        "Copy_to_Drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "if Copy_to_Drive :\n",
        "  !cp {downloaded_model_path} /content/drive/MyDrive/EveryDreamCkpts\n",
        "\n",
        "print(downloaded_model_path) # cache location\n",
        "print(finish_msg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Copy Model To Gdrive to save time in Following runs and space on your instance\n",
        "\n",
        "!cp /root/.cache/huggingface/hub/models--panopstor--EveryDream/snapshots/c768f08eebba76b42fcebba817f2643885202c81/sd_v1-5_vae.ckpt /content/drive/MyDrive/EveryDreamCkpts"
      ],
      "metadata": {
        "id": "YFlNM7rhgzz6"
      },
      "id": "YFlNM7rhgzz6",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FRD2wN4hPEl"
      },
      "id": "6FRD2wN4hPEl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0bf1e8cd",
      "metadata": {
        "id": "0bf1e8cd"
      },
      "source": [
        "# Upload training files\n",
        "\n",
        "Ues the navigation on the left to open **\"every-dream-trainer\"** folder, then open **\"input\"** and upload your training files using the **up arrow button** above the file explorer.  \n",
        "\n",
        "You can upload your images while you wait for the base checkpoint to download above, but do not continue until the checkpoint download is done.\n",
        "\n",
        "If your captions are in .txt files, upload those in the same folder as the images. \n",
        "\n",
        "![upload](https://github.com/victorchall/EveryDream-trainer/blob/main/demo/runpodupload.png?raw=1)\n",
        "\n",
        "You can check there are files in the folder by running the cell below (optional, just prints first 10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af711a9c-7a50-49a1-a571-439d62a9587c",
      "metadata": {
        "id": "af711a9c-7a50-49a1-a571-439d62a9587c"
      },
      "outputs": [],
      "source": [
        "!ls -U input | head -10\n",
        "# at least a few image filenames should show below, if not you uploaded to the wrong folder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "873d9f3f",
      "metadata": {
        "id": "873d9f3f"
      },
      "source": [
        "## Tweak your YAML\n",
        "You can adjust the YAML file to change the training parameters.  The v1-fine yamls are in the everydream-trainer/configs/stable-diffusion folder.  By default the \"v1-finetune_runpod.yaml\" is used here with some sane defaults for one subject with 20-30 images.\n",
        "\n",
        "You can also upload your own in that folder and use it below.\n",
        "\n",
        "Instructions are here: [EveryDream README](https://github.com/victorchall/EveryDream-trainer/blob/main/README.md) (hopefully you already read this?)\n",
        "\n",
        "[Runpod YAML](configs/stable-diffusion/v1-finetune_runpod.yaml) is a good starting point for small datasets (30-50 images) and **is the default in the command below**. It will only keep 2 checkpoints.\n",
        "\n",
        "If you are running on an A100 on Colab or otherwise, you can adjust the batch size up substantially.  Batch size 16 on A100 40GB as been tested as working.  In Colab, use the file navigation on the left to open the yaml.\n",
        "\n",
        "If you need help with larger training, join the Discord for advice on making a custom yaml."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "65e8a052-bc26-4339-81e0-3b4f78c1d286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "65e8a052-bc26-4339-81e0-3b4f78c1d286",
        "outputId": "b8df06a0-b1f9-4d96-ef23-ddf4aed5db2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Logs/input2022-12-04T01-38-09_test/checkpoints/Mitn=0_epoch=03-step=00900.ckpt\n",
            "yaml set to: configs/stable-diffusion/v1-finetune_runpod.yaml\n"
          ]
        }
      ],
      "source": [
        "#@markdown change to use a custom yaml, but RUN THIS CELL NO MATTER WHAT to set your yaml path\n",
        "\n",
        "Training_Script =\"runpod\" #@param [\"runpod\"] {allow-input: true}\n",
        "Custom_Model = True #@param {type:\"boolean\"}\n",
        "if Training_Script==\"runpod\" :\n",
        " my_yaml = \"configs/stable-diffusion/v1-finetune_runpod.yaml\"\n",
        "\n",
        "#elif...\n",
        "#all the elifs....\n",
        "\n",
        "#@markdown This can be a link on your Gdrive\n",
        "\n",
        "if Custom_Model :\n",
        "  downloaded_model_path =\"/content/drive/MyDrive/Logs/input2022-12-04T01-38-09_test/checkpoints/Mitn=0_epoch=03-step=00900.ckpt\" #@param {type:\"string\"}\n",
        "\n",
        "# downloaded_model_path = \"v1-5-pruned-emaonly.ckpt\"  # this is the default, but you can change it by uncommenting if you uploaded a file into /everydream-trainer\n",
        "print(downloaded_model_path) # reminder in case something went wrong with download\n",
        "print(f\"yaml set to: {my_yaml}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d909e7",
      "metadata": {
        "id": "c7d909e7"
      },
      "source": [
        "## Run the trainer\n",
        "Next cell runs training.  This will take a while depending on your number of images, repeats, and max_epochs.  \n",
        "\n",
        "You can watch for test images in the logs folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c12e7cf3-42be-4537-a4f7-5723c0248562",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c12e7cf3-42be-4537-a4f7-5723c0248562",
        "outputId": "8638c98c-a82c-4361-cff6-a8c918de7e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global seed set to 23\n",
            "Running on GPUs 0,\n",
            "Loading model from /content/drive/MyDrive/Logs/input2022-12-04T01-38-09_test/checkpoints/Mitn=0_epoch=03-step=00900.ckpt\n",
            "ckpt: /content/drive/MyDrive/Logs/input2022-12-04T01-38-09_test/checkpoints/Mitn=0_epoch=03-step=00900.ckpt has 900 steps\n",
            "Instantiating config for: ldm.models.diffusion.ddpm.LatentDiffusion with config:\n",
            "{'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'image', 'cond_stage_key': 'caption', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': True, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'unfreeze_model': True, 'model_lr': 1e-06, 'unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_spatial_transformer': True, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 512, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'ldm.modules.encoders.modules.FrozenCLIPEmbedder'}, 'ckpt_path': '/content/drive/MyDrive/Logs/input2022-12-04T01-38-09_test/checkpoints/Mitn=0_epoch=03-step=00900.ckpt'}\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "Instantiating config for: ldm.modules.diffusionmodules.openaimodel.UNetModel with config:\n",
            "{'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_spatial_transformer': True, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False}\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "Instantiating config for: ldm.models.autoencoder.AutoencoderKL with config:\n",
            "{'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 512, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Instantiating config for: torch.nn.Identity with config:\n",
            "{}\n",
            "Instantiating config for: ldm.modules.encoders.modules.FrozenCLIPEmbedder with config:\n",
            "{}\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'text_projection.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Restored from /content/drive/MyDrive/Logs/input2022-12-04T01-38-09_test/checkpoints/Mitn=0_epoch=03-step=00900.ckpt with 0 missing and 0 unexpected keys\n",
            "Instantiating config for: pytorch_lightning.loggers.TestTubeLogger with config:\n",
            "{'name': 'testtube', 'save_dir': '/content/drive/MyDrive/Logs/input2022-12-04T04-42-27_test'}\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/test_tube.py:105: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.\n",
            "  rank_zero_deprecation(\n",
            "Monitoring val/loss_simple_ema as checkpoint metric.\n",
            "Merged modelckpt-cfg: \n",
            "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': '/content/drive/MyDrive/Logs/input2022-12-04T04-42-27_test/checkpoints', 'filename': '{Mitn}_{epoch:02d}-{step:05d}', 'verbose': True, 'monitor': 'val/loss_simple_ema', 'every_n_epochs': 1, 'save_top_k': 3, 'save_last': False}}\n",
            "Instantiating config for: main.SetupCallback with config:\n",
            "{'resume': '', 'now': 'input2022-12-04T04-42-27', 'logdir': '/content/drive/MyDrive/Logs/input2022-12-04T04-42-27_test', 'ckptdir': '/content/drive/MyDrive/Logs/input2022-12-04T04-42-27_test/checkpoints', 'cfgdir': '/content/drive/MyDrive/Logs/input2022-12-04T04-42-27_test/configs', 'config': {'model': {'base_learning_rate': 1e-06, 'target': 'ldm.models.diffusion.ddpm.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'image', 'cond_stage_key': 'caption', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': True, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'unfreeze_model': True, 'model_lr': 1e-06, 'unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_spatial_transformer': True, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 512, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'ldm.modules.encoders.modules.FrozenCLIPEmbedder'}, 'ckpt_path': '/content/drive/MyDrive/Logs/input2022-12-04T01-38-09_test/checkpoints/Mitn=0_epoch=03-step=00900.ckpt'}}, 'data': {'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 16, 'num_workers': 16, 'wrap': 'falsegit', 'train': {'target': 'ldm.data.every_dream.EveryDreamBatch', 'params': {'repeats': 25, 'flip_p': 0, 'debug_level': 1}}, 'validation': {'target': 'ldm.data.ed_validate.EDValidateBatch', 'params': {'repeats': 10}}, 'test': {'target': 'ldm.data.ed_validate.EDValidateBatch', 'params': {'repeats': 1}}}}}, 'lightning_config': {'modelcheckpoint': {'params': {'every_n_epochs': 1, 'save_top_k': 3, 'save_last': False, 'filename': '{Mitn}_{epoch:02d}-{step:05d}'}}, 'callbacks': {'image_logger': {'target': 'main.ImageLogger', 'params': {'batch_frequency': 188, 'max_images': 16, 'increase_log_steps': False}}}, 'trainer': {'benchmark': True, 'max_epochs': 8, 'max_steps': 99000, 'check_val_every_n_epoch': 1, 'gpus': '0,'}}}\n",
            "Instantiating config for: main.ImageLogger with config:\n",
            "{'batch_frequency': 188, 'max_images': 16, 'clamp': True, 'increase_log_steps': False}\n",
            "Instantiating config for: main.LearningRateMonitor with config:\n",
            "{'logging_interval': 'step'}\n",
            "Instantiating config for: main.CUDACallback with config:\n",
            "{}\n",
            "Instantiating config for: pytorch_lightning.callbacks.ModelCheckpoint with config:\n",
            "{'dirpath': '/content/drive/MyDrive/Logs/input2022-12-04T04-42-27_test/checkpoints', 'filename': '{Mitn}_{epoch:02d}-{step:05d}', 'verbose': True, 'monitor': 'val/loss_simple_ema', 'every_n_epochs': 1, 'save_top_k': 3, 'save_last': False}\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Instantiating config for: main.DataModuleFromConfig with config:\n",
            "{'batch_size': 16, 'num_workers': 16, 'wrap': 'falsegit', 'train': {'target': 'ldm.data.every_dream.EveryDreamBatch', 'params': {'repeats': 25, 'flip_p': 0, 'debug_level': 1, 'data_root': 'input'}}, 'validation': {'target': 'ldm.data.ed_validate.EDValidateBatch', 'params': {'repeats': 10, 'data_root': 'input'}}, 'test': {'target': 'ldm.data.ed_validate.EDValidateBatch', 'params': {'repeats': 1, 'data_root': 'input'}}}\n",
            " ****** validation: {'target': 'ldm.data.ed_validate.EDValidateBatch', 'params': {'repeats': 10, 'data_root': 'input', 'batch_size': 16, 'set': 'val'}}\n",
            "Instantiating config for: ldm.data.every_dream.EveryDreamBatch with config:\n",
            "{'repeats': 25, 'flip_p': 0, 'debug_level': 1, 'data_root': 'input', 'batch_size': 16, 'set': 'train'}\n",
            " * Creating new dataloader singleton\n",
            "* DLMA resolution 512, buckets: [[512, 512], [576, 448], [448, 576], [640, 384], [384, 640], [768, 320], [320, 768], [832, 256], [256, 832], [896, 256], [256, 896], [960, 256], [256, 960], [1024, 256], [256, 1024]]\n",
            " Preloading images...\n",
            " ** Number of buckets: 7\n",
            "  ** Bucket (512, 512) with 136 will drop 8 images due to batch size 16\n",
            "  ** Bucket (576, 448) with 16 will drop 0 images due to batch size 16\n",
            "  ** Bucket (448, 576) with 5 will drop 5 images due to batch size 16\n",
            "  ** Bucket (384, 640) with 4 will drop 4 images due to batch size 16\n",
            "  ** Bucket (640, 384) with 12 will drop 12 images due to batch size 16\n",
            "  ** Bucket (320, 768) with 1 will drop 1 images due to batch size 16\n",
            "  ** Bucket (768, 320) with 2 will drop 2 images due to batch size 16\n",
            " * DLMA Example: <ldm.data.image_train_item.ImageTrainItem object at 0x7f7ce9e66d90> images\n",
            "\n",
            " ** Trainer Set: train, steps: 225, num_images: 144, batch_size: 16, length w/repeats: 3600\n",
            "\n",
            "Instantiating config for: ldm.data.ed_validate.EDValidateBatch with config:\n",
            "{'repeats': 10, 'data_root': 'input', 'batch_size': 16, 'set': 'val'}\n",
            "\n",
            " ** Validation Set: val, steps: 90, repeats: 10 \n",
            "\n",
            "Instantiating config for: ldm.data.ed_validate.EDValidateBatch with config:\n",
            "{'repeats': 1, 'data_root': 'input', 'batch_size': 16, 'set': 'test'}\n",
            "\n",
            " ** Validation Set: test, steps: 9, repeats: 1 \n",
            "\n",
            "Instantiating config for: ldm.data.every_dream.EveryDreamBatch with config:\n",
            "{'repeats': 25, 'flip_p': 0, 'debug_level': 1, 'data_root': 'input', 'batch_size': 16, 'set': 'train'}\n",
            "\n",
            " ** Trainer Set: train, steps: 225, num_images: 144, batch_size: 16, length w/repeats: 3600\n",
            "\n",
            "Instantiating config for: ldm.data.ed_validate.EDValidateBatch with config:\n",
            "{'repeats': 10, 'data_root': 'input', 'batch_size': 16, 'set': 'val'}\n",
            "\n",
            " ** Validation Set: val, steps: 90, repeats: 10 \n",
            "\n",
            "Instantiating config for: ldm.data.ed_validate.EDValidateBatch with config:\n",
            "{'repeats': 1, 'data_root': 'input', 'batch_size': 16, 'set': 'test'}\n",
            "\n",
            " ** Validation Set: test, steps: 9, repeats: 1 \n",
            "\n",
            "#### Data #####\n",
            "train, WrappedDataset, 3600\n",
            "validation, WrappedDataset, 1440\n",
            "test, WrappedDataset, 144\n",
            "accumulate_grad_batches = 1\n",
            "++++ NOT USING LR SCALING ++++\n",
            "Setting learning rate to 1.00e-06\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:326: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:342: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
            "  rank_zero_deprecation(\n",
            "Instantiating config for: ldm.data.every_dream.EveryDreamBatch with config:\n",
            "{'repeats': 25, 'flip_p': 0, 'debug_level': 1, 'data_root': 'input', 'batch_size': 16, 'set': 'train'}\n",
            "\n",
            " ** Trainer Set: train, steps: 225, num_images: 144, batch_size: 16, length w/repeats: 3600\n",
            "\n",
            "Instantiating config for: ldm.data.ed_validate.EDValidateBatch with config:\n",
            "{'repeats': 10, 'data_root': 'input', 'batch_size': 16, 'set': 'val'}\n",
            "\n",
            " ** Validation Set: val, steps: 90, repeats: 10 \n",
            "\n",
            "Instantiating config for: ldm.data.ed_validate.EDValidateBatch with config:\n",
            "{'repeats': 1, 'data_root': 'input', 'batch_size': 16, 'set': 'test'}\n",
            "\n",
            " ** Validation Set: test, steps: 9, repeats: 1 \n",
            "\n",
            "Instantiating config for: ldm.data.every_dream.EveryDreamBatch with config:\n",
            "{'repeats': 25, 'flip_p': 0, 'debug_level': 1, 'data_root': 'input', 'batch_size': 16, 'set': 'train'}\n",
            "\n",
            " ** Trainer Set: train, steps: 225, num_images: 144, batch_size: 16, length w/repeats: 3600\n",
            "\n",
            "Instantiating config for: ldm.data.ed_validate.EDValidateBatch with config:\n",
            "{'repeats': 10, 'data_root': 'input', 'batch_size': 16, 'set': 'val'}\n",
            "\n",
            " ** Validation Set: val, steps: 90, repeats: 10 \n",
            "\n",
            "Instantiating config for: ldm.data.ed_validate.EDValidateBatch with config:\n",
            "{'repeats': 1, 'data_root': 'input', 'batch_size': 16, 'set': 'test'}\n",
            "\n",
            " ** Validation Set: test, steps: 9, repeats: 1 \n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "LatentDiffusion: Also optimizing conditioner params!\n",
            "\n",
            "  | Name              | Type               | Params\n",
            "---------------------------------------------------------\n",
            "0 | model             | DiffusionWrapper   | 859 M \n",
            "1 | first_stage_model | AutoencoderKL      | 83.7 M\n",
            "2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
            "---------------------------------------------------------\n",
            "982 M     Trainable params\n",
            "83.7 M    Non-trainable params\n",
            "1.1 B     Total params\n",
            "4,264.941 Total estimated model params size (MB)\n",
            "2022-12-04 04:43:17.272506: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Project config\n",
            "model:\n",
            "  base_learning_rate: 1.0e-06\n",
            "  target: ldm.models.diffusion.ddpm.LatentDiffusion\n",
            "  params:\n",
            "    linear_start: 0.00085\n",
            "    linear_end: 0.012\n",
            "    num_timesteps_cond: 1\n",
            "    log_every_t: 200\n",
            "    timesteps: 1000\n",
            "    first_stage_key: image\n",
            "    cond_stage_key: caption\n",
            "    image_size: 64\n",
            "    channels: 4\n",
            "    cond_stage_trainable: true\n",
            "    conditioning_key: crossattn\n",
            "    monitor: val/loss_simple_ema\n",
            "    scale_factor: 0.18215\n",
            "    use_ema: false\n",
            "    unfreeze_model: true\n",
            "    model_lr: 1.0e-06\n",
            "    unet_config:\n",
            "      target: ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
            "      params:\n",
            "        image_size: 32\n",
            "        in_channels: 4\n",
            "        out_channels: 4\n",
            "        model_channels: 320\n",
            "        attention_resolutions:\n",
            "        - 4\n",
            "        - 2\n",
            "        - 1\n",
            "        num_res_blocks: 2\n",
            "        channel_mult:\n",
            "        - 1\n",
            "        - 2\n",
            "        - 4\n",
            "        - 4\n",
            "        num_heads: 8\n",
            "        use_spatial_transformer: true\n",
            "        transformer_depth: 1\n",
            "        context_dim: 768\n",
            "        use_checkpoint: true\n",
            "        legacy: false\n",
            "    first_stage_config:\n",
            "      target: ldm.models.autoencoder.AutoencoderKL\n",
            "      params:\n",
            "        embed_dim: 4\n",
            "        monitor: val/rec_loss\n",
            "        ddconfig:\n",
            "          double_z: true\n",
            "          z_channels: 4\n",
            "          resolution: 512\n",
            "          in_channels: 3\n",
            "          out_ch: 3\n",
            "          ch: 128\n",
            "          ch_mult:\n",
            "          - 1\n",
            "          - 2\n",
            "          - 4\n",
            "          - 4\n",
            "          num_res_blocks: 2\n",
            "          attn_resolutions: []\n",
            "          dropout: 0.0\n",
            "        lossconfig:\n",
            "          target: torch.nn.Identity\n",
            "    cond_stage_config:\n",
            "      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder\n",
            "    ckpt_path: /content/drive/MyDrive/Logs/input2022-12-04T01-38-09_test/checkpoints/Mitn=0_epoch=03-step=00900.ckpt\n",
            "data:\n",
            "  target: main.DataModuleFromConfig\n",
            "  params:\n",
            "    batch_size: 16\n",
            "    num_workers: 16\n",
            "    wrap: falsegit\n",
            "    train:\n",
            "      target: ldm.data.every_dream.EveryDreamBatch\n",
            "      params:\n",
            "        repeats: 25\n",
            "        flip_p: 0\n",
            "        debug_level: 1\n",
            "    validation:\n",
            "      target: ldm.data.ed_validate.EDValidateBatch\n",
            "      params:\n",
            "        repeats: 10\n",
            "    test:\n",
            "      target: ldm.data.ed_validate.EDValidateBatch\n",
            "      params:\n",
            "        repeats: 1\n",
            "\n",
            "Lightning config\n",
            "modelcheckpoint:\n",
            "  params:\n",
            "    every_n_epochs: 1\n",
            "    save_top_k: 3\n",
            "    save_last: false\n",
            "    filename: '{Mitn}_{epoch:02d}-{step:05d}'\n",
            "callbacks:\n",
            "  image_logger:\n",
            "    target: main.ImageLogger\n",
            "    params:\n",
            "      batch_frequency: 188\n",
            "      max_images: 16\n",
            "      increase_log_steps: false\n",
            "trainer:\n",
            "  benchmark: true\n",
            "  max_epochs: 8\n",
            "  max_steps: 99000\n",
            "  check_val_every_n_epoch: 1\n",
            "  gpus: 0,\n",
            "\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "#@title # Run the trainer, wait until it finishes then SCROLL DOWN to the next cell\n",
        "#@markdown Choose your paramaters \n",
        "\n",
        "\n",
        "\n",
        "Feeling_Lucky = False #@param {type:\"boolean\"}\n",
        "#@markdown * # DELETES 11gb files after prune!!\n",
        "\n",
        "Disconnect_after_transfer = True #@param {type:\"boolean\"}\n",
        "bye =\"\"\n",
        "#@markdown * close the Colab once training ends to save credits\n",
        "\n",
        "save_logs_to_Drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown * save your logs to the Gdrive during training this includes\n",
        "#@markdown all images and .ckpt and configs\n",
        "\n",
        "Transfer_ckpt = True #@param {type:\"boolean\"}\n",
        "#@markdown * move your ckpt finished ckpts to everydreamckpts on gdrive\n",
        "\n",
        "\n",
        "Source_images = \"\" #@param {type:\"string\"}\n",
        "#@markdown * leave blank to upload your images to the session inputs manualy\n",
        "\n",
        "#@markdown * place the drirectory of your images from the Gdrive\n",
        "if Source_images== \"\":\n",
        "  Source =\"input\"\n",
        "else:\n",
        "  Source =Source_images\n",
        "\n",
        "slog =\"\"\n",
        "Model_in_trainings_name = \"Family_guy\" #@param {type:\"string\"}\n",
        "Mitn =\"\"\n",
        "Mitn = Model_in_trainings_name\n",
        "if Feeling_Lucky :\n",
        "   bye =\" --delete\"\n",
        "if save_logs_to_Drive :\n",
        "   slog= \"--logdir /content/drive/MyDrive/Logs\"\n",
        "\n",
        "!python main.py --base {my_yaml} -t  --actual_resume {downloaded_model_path} -n test --data_root $Source $slog\n",
        "\n",
        "if Transfer_ckpt:\n",
        "  if save_logs_to_Drive:\n",
        "    !cd /content/drive/MyDrive/Logs\n",
        "    !python scripts/autoprune_all.py $bye\n",
        "    !cp *.ckpt /content/drive/MyDrive/EveryDreamCkpts\n",
        "  else :\n",
        "      !python scripts/autoprune_all.py $bye\n",
        "      !cp *.ckpt /content/drive/MyDrive/EveryDreamCkpts\n",
        "\n",
        "if Disconnect_after_transfer :\n",
        "  runtime.unassign()\n",
        "\n",
        "print(finish_msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c93085",
      "metadata": {
        "id": "e8c93085"
      },
      "source": [
        "## Prune your checkpoints\n",
        "This will create 2GB pruned files for all your checkpoints and delete the 11GB files.\n",
        "\n",
        "If you wish to resume, you may want to remove \"--delete\" command below and manually delete the 11GB files you dont want to keep.  Typically you only need the last 1 checkpoint in 11gb for resuming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e70ae7e",
      "metadata": {
        "id": "5e70ae7e"
      },
      "outputs": [],
      "source": [
        "# prune the ckpts\n",
        "!python scripts/autoprune_all.py --delete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065dfd91",
      "metadata": {
        "id": "065dfd91"
      },
      "source": [
        "## Test\n",
        "Look in the file drawer on the left for your epoch ckpt names and try them out in the cell below one at a time.  You can save time just downloading the one pruned file that looks the best.  Try each out.\n",
        "\n",
        "Change the prompt and the ckpt_path below to appropriate values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "954dfc20",
      "metadata": {
        "id": "954dfc20"
      },
      "outputs": [],
      "source": [
        "!python scripts/txt2img.py --ckpt_path \"epoch=02-step=01000.ckpt\" \\\n",
        "--n_samples 2 \\\n",
        "--n_iter 4 \\\n",
        "--prompt \"a boy and his dog talking a walk down the sidewalk\" \\\n",
        "--scale 6.0 \\\n",
        "--outdir outputs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13533d6",
      "metadata": {
        "id": "e13533d6"
      },
      "outputs": [],
      "source": [
        "# run to show the images here, or use the file drawer on the left to look at them\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "for imageName in glob.glob('outputs/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(imageName)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51456afe",
      "metadata": {
        "id": "51456afe"
      },
      "source": [
        "## Download your checkpoints\n",
        "\n",
        "Use the cell below to generate links, right click and save as to download.  If you use Colab, you can skip this and use the Gdrive connect instead.\n",
        "\n",
        "**If the links don't work, you can double left click the ckpt file in the file drawer on the left, then go to \"File\" menu then \"Download\"** or use the Hugging Face upload below.\n",
        "\n",
        "[EveryDream Discord](https://discord.gg/uheqxU6sXN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f94f9ab",
      "metadata": {
        "id": "1f94f9ab"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import FileLink\n",
        "for f in glob.glob(\"*.ckpt\"):\n",
        "    display(FileLink(f))\n",
        "# right click save as to download the ckpt files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13d37a1",
      "metadata": {
        "id": "c13d37a1"
      },
      "source": [
        "# HuggingFace upload\n",
        "Use the cell below to upload your checkpoint to your personal HuggingFace account if you want instead of manually downloading. You should already be authorized to Huggingface by token if you used the download/token cells above.\n",
        "\n",
        "Make sure to fill in the three fields at the top. This will only upload one file at a time, so you will need to run the cell below for each file you want to upload.\n",
        "\n",
        "* You can get your account name from your [HuggingFace account page](https://huggingface.co/settings/account). Look for your \"username\" field and paste it below.\n",
        "\n",
        "* You only need to setup a repository one time.  You can create it here: [Create New HF Dataset](https://huggingface.co/new)  Make sure you write down the repo name you make for future use.  You can reuse it later.\n",
        "\n",
        "* You need to type the name of the ckpts one at a time in the cell below, find them in the left file drawer of your Runpod/Vast/Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb962e0",
      "metadata": {
        "id": "9fb962e0"
      },
      "outputs": [],
      "source": [
        "#list ckpts in root that are ready for download\n",
        "import glob\n",
        "\n",
        "for f in glob.glob(\"*.ckpt\"):\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7237ed4",
      "metadata": {
        "id": "f7237ed4"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli lfs-enable-largefiles\n",
        "# fill in these three fields:\n",
        "hfusername = \"panopstor\"\n",
        "reponame = \"EveryDream\"\n",
        "ckpt_name = \"epoch=01-step=00030-pruned.ckpt\"\n",
        "\n",
        "\n",
        "target_name = ckpt_name.replace('-','').replace('=','')\n",
        "import os\n",
        "os.rename(ckpt_name,target_name)\n",
        "repo_id=f\"{hfusername}/{reponame}\"\n",
        "print(f\"uploading to HF: {repo_id}/{ckpt_name}\")\n",
        "print(\"this make take a while...\")\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "response = api.upload_file(\n",
        "    path_or_fileobj=target_name,\n",
        "    path_in_repo=target_name,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=None,\n",
        "    create_pr=1,\n",
        ")\n",
        "print(response)\n",
        "print(finish_msg)\n",
        "print(\"go to your repo and accept the PR this created to see your file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54171a12",
      "metadata": {
        "id": "54171a12"
      },
      "source": [
        "# Gdrive connect\n",
        "\n",
        "For Colab only, copies your ckpts to your gdrive.  If the EveryDreamCkpts folder already exists in your gdrive there will be an error, but it should still copy your files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857015ff",
      "metadata": {
        "id": "857015ff"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir /content/drive/MyDrive/EveryDreamCkpts\n",
        "!cp *.ckpt /content/drive/MyDrive/EveryDreamCkpts"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2e677f113ff5b533036843965d6e18980b635d0aedc1c5cebd058006c5afc92a"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "920086eff21248e48d39e1445d8f9856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83faf10b9bce4a269f453a5d50706d51",
              "IPY_MODEL_4b32a734fcfa4813a323e67043dda0a2",
              "IPY_MODEL_c662c77edca44e2fb34f88bc8b1944e3",
              "IPY_MODEL_513ae98436d144978242c38d18553058",
              "IPY_MODEL_cc2e8824c1a14226925ba9b6529a2d54"
            ],
            "layout": "IPY_MODEL_0c7e2e32f6bc49fa9d93b1cb75f6324f"
          }
        },
        "83faf10b9bce4a269f453a5d50706d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1026949927f648d18634a5f1f248e694",
            "placeholder": "​",
            "style": "IPY_MODEL_b89c77e6c2e5444e8b1642187cf6d7c2",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "4b32a734fcfa4813a323e67043dda0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9c0a87e8ed0b4f3e9c30f385228084da",
            "placeholder": "​",
            "style": "IPY_MODEL_50e47d3d1b984424b06a9f38d83e7a69",
            "value": ""
          }
        },
        "c662c77edca44e2fb34f88bc8b1944e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_429e9e25849a48fd8b12469f14c434f6",
            "style": "IPY_MODEL_9ac4369ef42c4d329e4c3d674bf00f62",
            "value": true
          }
        },
        "513ae98436d144978242c38d18553058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_af661d4d10ea4ebbbab9e9e9df9ccfe4",
            "style": "IPY_MODEL_d5b3d673fa92457bb818f04e98da5203",
            "tooltip": ""
          }
        },
        "cc2e8824c1a14226925ba9b6529a2d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dae2a73653c54c189a78cf0379f083a7",
            "placeholder": "​",
            "style": "IPY_MODEL_197d679123f14f21836c6a7ac49f04ce",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "0c7e2e32f6bc49fa9d93b1cb75f6324f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1026949927f648d18634a5f1f248e694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89c77e6c2e5444e8b1642187cf6d7c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c0a87e8ed0b4f3e9c30f385228084da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50e47d3d1b984424b06a9f38d83e7a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "429e9e25849a48fd8b12469f14c434f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac4369ef42c4d329e4c3d674bf00f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af661d4d10ea4ebbbab9e9e9df9ccfe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5b3d673fa92457bb818f04e98da5203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "dae2a73653c54c189a78cf0379f083a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "197d679123f14f21836c6a7ac49f04ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}